{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from kaggle import data, nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv = data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vec=CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tags=vec.fit_transform(csv['tags']).todense()\n",
    "# tags=pd.DataFrame(tags, columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- CC\tcoordinating conjunction\n",
    "- CD\tcardinal digit\n",
    "- DT\tdeterminer\n",
    "- EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "- FW\tforeign word\n",
    "- IN\tpreposition/subordinating conjunction\n",
    "- JJ\tadjective\t'big'\n",
    "- JJR\tadjective, comparative\t'bigger'\n",
    "- JJS\tadjective, superlative\t'biggest'\n",
    "- LS\tlist marker\t1)\n",
    "- MD\tmodal\tcould, will\n",
    "- NN\tnoun, singular 'desk'\n",
    "- NNS\tnoun plural\t'desks'\n",
    "- NNP\tproper noun, singular\t'Harrison'\n",
    "- NNPS\tproper noun, plural\t'Americans'\n",
    "- PDT\tpredeterminer\t'all the kids'\n",
    "- POS\tpossessive ending\tparent's\n",
    "- PRP\tpersonal pronoun\tI, he, she\n",
    "- PRPS\tpossessive pronoun\tmy, his, hers\n",
    "- RB\tadverb\tvery, silently,\n",
    "- RBR\tadverb, comparative\tbetter\n",
    "- RBS\tadverb, superlative\tbest\n",
    "- RP\tparticle\tgive up\n",
    "- TO\tto\tgo 'to' the store.\n",
    "- UH\tinterjection\terrrrrrrrm\n",
    "- VB\tverb, base form\ttake\n",
    "- VBD\tverb, past tense\ttook\n",
    "- VBG\tverb, gerund/present participle\ttaking\n",
    "- VBN\tverb, past participle\ttaken\n",
    "- VBP\tverb, sing. present, non-3d\ttake\n",
    "- VBZ\tverb, 3rd person sing. present\ttakes\n",
    "- WDT\twh-determiner\twhich\n",
    "- WP\twh-pronoun\twho, what\n",
    "- WPS\tpossessive wh-pronoun\twhose\n",
    "- WRB\twh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, PunktSentenceTokenizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title1 = csv.iloc[0, 0]\n",
    "title2 = csv.iloc[1, 0]\n",
    "title3 = csv.iloc[2, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence1 = csv.iloc[0, 1]\n",
    "sentence2 = csv.iloc[1, 1]\n",
    "sentence3 = csv.iloc[2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags1 = csv.iloc[0, 2]\n",
    "tags2 = csv.iloc[1, 2]\n",
    "tags3 = csv.iloc[2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print sentence2, \"\\n\"\n",
    "# print tags2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenizer = PunktSentenceTokenizer(sentence1)\n",
    "# tokenized = tokenizer.tokenize(sentence2)\n",
    "# tokenized = sent_tokenize(sentence2, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"kaggle/long_stopwords.txt\") as f:\n",
    "    stopwords = [word for line in f for word in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def break_list_of_tags(list_of_tags):\n",
    "    unique_tags = set()\n",
    "    for tagline in list_of_tags:\n",
    "        for word in tagline.split(\" \"):\n",
    "            if word not in unique_tags:\n",
    "                unique_tags.add(word)\n",
    "    return list(unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = list(csv.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(values):\n",
    "    output = []\n",
    "    seen = set()\n",
    "    for value in values:\n",
    "        # If value has not been encountered yet,\n",
    "        # ... add it to both list and set.\n",
    "        if value not in seen:\n",
    "            output.append(value)\n",
    "            seen.add(value)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def process_content(sentence, stopwords, all_tags):\n",
    "#     tokens = sent_tokenize(sentence, language='english')\n",
    "    \n",
    "#     cp = nltk.RegexpParser(r\"\"\"chunk: {<NNP>*<NN>*}\"\"\")\n",
    "#     chunks=[]\n",
    "#     for sent in tokens:\n",
    "#         tree = cp.parse(pos_tag(word_tokenize(sent)))\n",
    "#         for subtree in tree.subtrees():\n",
    "#             if subtree.label() == 'chunk': \n",
    "#                 chunks.append(subtree)\n",
    "                \n",
    "# #     return chunks\n",
    "#     tags=[]\n",
    "#     for tree in chunks:\n",
    "#         if len(tree.leaves()) == 1:\n",
    "#             if tree.leaves()[0][0] not in stopwords:\n",
    "#                 tag = tree.leaves()[0][0]\n",
    "#                 if tag in all_tags:\n",
    "#                     tags.append( tag)\n",
    "#         else:\n",
    "#             tags.append(\"-\".join([t[0] for t in tree.leaves()]))\n",
    "#     tags = remove_duplicates(tags)\n",
    "#     return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_content(sentence, stopwords, all_tags):\n",
    "    tokens = sent_tokenize(sentence, language='english')\n",
    "    \n",
    "    cp = nltk.RegexpParser(r\"\"\"chunk: {<NNP>*<NNPS>*<NN>*<NNS>*}\"\"\")\n",
    "    chunks=[]\n",
    "    for sent in tokens:\n",
    "        tree = cp.parse(pos_tag(word_tokenize(sent)))\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'chunk': \n",
    "                chunks.append(subtree)\n",
    "                \n",
    "#     return chunks\n",
    "    tags=[]\n",
    "    for tree in chunks:\n",
    "        if len(tree.leaves()) == 1:\n",
    "            if tree.leaves()[0][0] not in stopwords: tags.append(tree.leaves()[0][0]) \n",
    "        else:\n",
    "            tags.append(\"-\".join([t[0] for t in tree.leaves()]))\n",
    "    tags\n",
    "#     return remove_duplicates(tags)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"kaggle/long_stopwords.txt\") as f:\n",
    "    stopwords = [word for line in f for word in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence1 = sentence1.replace(\"  \", \" \")\n",
    "sentence1 = sentence1.replace(\"Thank\", \"thank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trees = process_content(\" \".join([sentence1, title1]), stopwords, tags)\n",
    "print title1, \"\\n\"\n",
    "print sentence1, \"\\n\"\n",
    "print tags1\n",
    "trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trees = process_content(\" \".join([sentence1, title1]), stopwords, tags)\n",
    "print title1, \"\\n\"\n",
    "print sentence1, \"\\n\"\n",
    "print tags1\n",
    "trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trees = process_content(\" \".join([sentence2, title2]), stopwords, tags)\n",
    "print title2, \"\\n\"\n",
    "print sentence2, \"\\n\"\n",
    "print tags2\n",
    "trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# title3 = title3.replace(\"?\", '')\n",
    "# sentence3 = sentence3.replace(\"?\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trees = process_content(\" \".join([sentence3, title3]), stopwords, tags)\n",
    "print title3, \"\\n\"\n",
    "print sentence3, \"\\n\"\n",
    "print tags3\n",
    "trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spam = \" \".join(process_content(\" \".join([sentence1, title1]), stopwords, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ham = \" \".join(list(csv.tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tvec = TfidfVectorizer(stop_words='english')\n",
    "# tvec.fit([spam, ham])\n",
    "\n",
    "# df  = pd.DataFrame(tvec.transform([spam, ham]).todense(),\n",
    "#                    columns=tvec.get_feature_names(),\n",
    "#                    index=['spam', 'ham'])\n",
    "\n",
    "# df.transpose().sort_values('spam', ascending=False).head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv['gen_tagg'] = None\n",
    "for i in range(len(csv)):\n",
    "    sentence = \" \".join([csv.loc[i, 'content'], csv.loc[i, 'title']])\n",
    "    \n",
    "    csv.loc[i, 'gen_tag'] = \" \".join(nlp.process_content(sentence, stopwords))\n",
    "    csv.loc[i, 'gen_tag_unique'] = \" \".join(remove_duplicates(nlp.process_content(sentence, stopwords)))\n",
    "#     csv.loc[i, 'gen_tag'] = process_content(\" \".join([csv.loc[i, 'content'], csv.loc[i, 'title']]), stopwords, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv.to_csv('dataset/all_tagged.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv = pd.read_csv('dataset/all_tagged.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_tfidf(row, data, filename, tag_count=6):\n",
    "    relevant = data[(data.file_name == filename)]\n",
    "    irrelevant = data[(data.file_name != filename)]\n",
    "    \n",
    "    hams={}\n",
    "    hams['tags'] = \" \".join(list(irrelevant.tags))\n",
    "    hams['generated_tags'] = \" \".join(list(irrelevant.gen_tag))\n",
    "    hams['unique_generated_tags'] = \" \".join(list(irrelevant.gen_tag_unique))\n",
    "    \n",
    "    spam = data.loc[row, 'gen_tag']\n",
    "    print data.loc[row, 'title'], '\\n'\n",
    "    print data.loc[row, 'content'], '\\n'\n",
    "    print cooking.loc[row, 'tags'], '\\n'\n",
    "    print cooking.loc[row, 'gen_tag'], '\\n'\n",
    "\n",
    "    for key, ham in hams.iteritems():\n",
    "        tvec = TfidfVectorizer(stop_words='english', token_pattern=r'[^\\s]+')\n",
    "        tvec.fit([spam, ham])\n",
    "\n",
    "        df  = pd.DataFrame(tvec.transform([spam, ham]).todense(),\n",
    "                           columns=tvec.get_feature_names(),\n",
    "                           index=['spam', 'ham'])\n",
    "        print key, \"\\n\"\n",
    "        print df.transpose().sort_values('spam', ascending=False).head(tag_count).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do_tfidf(1, csv, 'cooking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do_tfidf(2, csv, 'cooking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do_tfidf(3, csv, 'cooking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
